{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulating FTPs/SFTPs with Airflow.\n",
    "\n",
    "## Agenda:\n",
    "- Basic Airflow architecture.\n",
    "\n",
    "- Simple FTP requests\n",
    "\n",
    "- PythonOperator\n",
    "\n",
    "- Hook+Python Operator\n",
    "\n",
    "- Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow words.\n",
    "\n",
    "- DAG: Directed Acyclic Graph, the structure Airflow uses for its workflows. Each DAG has an ordering (one task can depend on another (Directed)) and contains no cycles (acyclic). A DAG is made up of seperate tasks that are the  configuration for the workflow's structure - all the heavy lifting is done in the hooks and operators.\n",
    "\n",
    "- Hooks: Files used by Airflow to interact with external systems (databases, APIs, etc.)\n",
    "\n",
    "- Operators: The atomic unit of logic in Airflow - these files determine how the work gets done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple FTP requests:\n",
    "\n",
    "#### There are plenty of different modules for dealing with FTPs in Python. For SFTPs, _paramiko_ is the best library to use.\n",
    "#### This unsecured example uses ftplib.\n",
    "\n",
    "_Note_: For . All commands used here have a paramiko equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Suppose you have some workflow that downloads data off of an FTP, does osme transformation, and uploads \n",
    "## it that runs on a cron schedule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ftplib import FTP\n",
    "\n",
    "def download_file(connection, file_name):\n",
    "    \"\"\"\n",
    "    Downloads file from FTP.\n",
    "    \"\"\"\n",
    "\n",
    "    filename = 'sample_data.csv'\n",
    "\n",
    "    localfile = open(filename, 'wb')\n",
    "    ftp.retrbinary('RETR ' + filename, localfile.write, 1024)\n",
    "\n",
    "    ftp.quit()\n",
    "    localfile.close()\n",
    "    \n",
    "    #Do some manipulations to the local file. \n",
    "    \n",
    "\n",
    "def upload_file(connection, file_name):\n",
    "    \"\"\"\n",
    "    Uploads file as binary to FTP. \n",
    "    \"\"\"\n",
    "    filename = 'sample_data.csv'\n",
    "    ftp.storbinary('STOR '+filename, open(filename, 'rb'))\n",
    "    ftp.quit()\n",
    "    \n",
    "host = ''\n",
    "username = ''\n",
    "password = ''\n",
    "port = 21\n",
    "\n",
    "ftp = FTP(host)\n",
    "connection = ftp.login(username, password)\n",
    "\n",
    "\n",
    "download_file(connection, 'test.csv')\n",
    "upload_file(connection, 'test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing this in Airflow:\n",
    "\n",
    "To do this in a DAG, we can use the PythonOperator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Airflow specific dependencies. \n",
    "from airflow import DAG\n",
    "from airflow.operators import DummyOperator\n",
    "from airflow.operators.python_operator import PythonOperator \n",
    "from datetime import datetime\n",
    "\n",
    "#Import the module for the FTP.. \n",
    "from ftplib import FTP\n",
    "\n",
    "\n",
    "#Define functions\n",
    "def upload_file(**kwargs):\n",
    "    \"\"\"\n",
    "    Uploads file as binary to FTP. \n",
    "    \"\"\"\n",
    "    \n",
    "    credentials = kwargs.get('templates_dict').get('credentials', None)\n",
    "    host = credentials['host']\n",
    "    username = credentials['username']\n",
    "    password = credentials['password']\n",
    "    \n",
    "    \n",
    "    ftp = FTP(host)\n",
    "    ftp.login(username, password)\n",
    "\n",
    "    \n",
    "    filename = 'sample_data.csv'\n",
    "    ftp.storbinary('STOR '+filename, open(filename, 'rb'))\n",
    "    ftp.quit()\n",
    "\n",
    "\n",
    "def download_file(**kwargs):\n",
    "    \"\"\"\n",
    "    Downloads file from FTP.\n",
    "    \"\"\"\n",
    "    \n",
    "    credentials = kwargs.get('templates_dict').get('credentials', None)\n",
    "    host = credentials['host']\n",
    "    username = credentials['username']\n",
    "    password = credentials['password']\n",
    "    \n",
    "\n",
    "    ftp = FTP(host)\n",
    "    ftp.login(username, password)\n",
    "\n",
    "    filename = 'sample_data.csv'\n",
    "\n",
    "    localfile = open(filename, 'wb')\n",
    "    ftp.retrbinary('RETR ' + filename, localfile.write, 1024)\n",
    "\n",
    "    ftp.quit()\n",
    "    localfile.close()\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime(2017, 12, 19)\n",
    "}\n",
    "\n",
    "\n",
    "# Schedule this DAG to run once.\n",
    "dag = DAG('test_ftp',\n",
    "          description='Manipulating FTPs with PythonOperators',\n",
    "          schedule_interval='@once',\n",
    "          start_date=datetime(2017, 12, 18),\n",
    "          default_args=default_args)\n",
    "\n",
    "# FTP creds\n",
    "credentials= {\n",
    "    'host' : ''\n",
    "    'username' : '',\n",
    "    'password' : '',\n",
    "    'port' : 21\n",
    "    \n",
    "}\n",
    "with dag:\n",
    "    # Dummy start DAG.\n",
    "    kick_off_dag = DummyOperator(task_id='kick_off_dag')\n",
    "\n",
    "    # Call the functions\n",
    "\n",
    "    download_file = PythonOperator(\n",
    "        task_id='download_file',\n",
    "        python_callable=download_file,\n",
    "        # This passes the date into the function as a dictionaryt.\n",
    "        templates_dict={'credentials': credentials},\n",
    "        provide_context=True\n",
    "    )\n",
    "    \n",
    "    upload_file = PythonOperator(\n",
    "    task_id='upload_file',\n",
    "    python_callable=upload_file, #function-name\n",
    "    # This passes the params into the function as a dictionaryt.\n",
    "    templates_dict={'credentials': credentials},\n",
    "    provide_context=True\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Set dependencies.First the kickoff, then the download, and finally, the upload.\n",
    "    # A task won't start until the one before it does.\n",
    "    # e.g. the upload won't start until the download taks finishes. \n",
    "    kick_off_dag >>  download_file >> upload_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why use the PythonOperator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airflow is made up of 3 core components: the webserver, the scheduler, and the executor.\n",
    "    \n",
    "    Webserver - Responsible for the UI in the browser.\n",
    "    Scheduler - Handles the scheduling and state of tasks.\n",
    "    Executor- Handles actually executing underlying code.\n",
    "    \n",
    "The scheduler \"heartbeats\" DAG files every few seconds before sending anything anything to the executor.\n",
    "Each \"heartbeat\" executes **all** top level code. Any code that isn't wrappped in an operator is executed \n",
    "each heartbeat, making it incredibly expensive.\n",
    "\n",
    "**Airflow Best Practice: Minimize top-level code. **\n",
    "    \n",
    "The PythonOperator is a quick and dirty way around this - just throw your function in a PythonOperator and you \n",
    "can leverage Airflow's scheduling and dependency capabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lots of repeated code.\n",
    "\n",
    "Python Operators make it easy to take previous scripts and easily schedule them with Airflow, but they lead to a \n",
    "fair deal of repeated code and aren't anymore modular than regular python functions. Furthermore, they make \n",
    "the DAG file itself cluttered. \n",
    "\n",
    "** Airflow Best Practice: The DAG file should be as close to a \"config\" file as possible. **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the FTP Hook.\n",
    "\n",
    "Using a hook to handle the connection can clean this code up a ton by handling the connection to the FTP.\n",
    "\n",
    "https://github.com/apache/incubator-airflow/blob/v1-8-stable/airflow/contrib/hooks/ftp_hook.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators import DummyOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime\n",
    "from airflow.contrib.hooks import FTPHook\n",
    "\n",
    "\n",
    "def upload_file(**kwargs):\n",
    "    \"\"\"\n",
    "    Uploads file as binary to FTP. \n",
    "    \"\"\"\n",
    "\n",
    "    hook = FTP(ftp_conn_id='astro_ftp').get_conn()\n",
    "\n",
    "    local_path = 'sample_data.csv'\n",
    "    remote_path = '/astro_test/saple_data.csv'\n",
    "    \n",
    "    hook.store_file(local_path, remote_path)\n",
    "    hook.close()\n",
    "\n",
    "\n",
    "def download_file(**kwargs):\n",
    "    \"\"\"\n",
    "    Downloads file from FTP.\n",
    "    \"\"\"\n",
    "    hook = FTP(ftp_conn_id='astro_ftp').get_conn()\n",
    "\n",
    "    local_path = 'sample_data.csv'\n",
    "    remote_path = '/astro_test/saple_data.csv'\n",
    "    \n",
    "    hook.retrieve_file(local_path, remote_path)\n",
    "    hook.close()\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime(2017, 12, 19)\n",
    "}\n",
    "\n",
    "\n",
    "# Schedule this DAG to run once.\n",
    "dag = DAG('test_ftp',\n",
    "          description='Manipulating FTPs with PythonOperators+Hooks',\n",
    "          schedule_interval='@once',\n",
    "          start_date=datetime(2017, 12, 18),\n",
    "          default_args=default_args)\n",
    "\n",
    "with dag:\n",
    "\n",
    "    kick_off_dag = DummyOperator(task_id='kick_off_dag')\n",
    "\n",
    "    upload_file = PythonOperator(\n",
    "        task_id='upload_file',\n",
    "        python_callable=upload_file,\n",
    "        # This passes the params into the function.\n",
    "        provide_context=True\n",
    "    )\n",
    "    \n",
    "    download_file = PythonOperator(\n",
    "        task_id='download_file',\n",
    "        python_callable=download_file,\n",
    "        # This passes the date into the function.\n",
    "        provide_context=True\n",
    "    )\n",
    "    \n",
    "    kick_off_dag >> upload_file >> download_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Connections.\n",
    "\n",
    "Notice when the hook was instanstiated, it was simply passed the name of the connection instead of the actual credentials used. All hooks inherit from the BaseHook, which has access to the Airflow database that stores connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![connections](img/airflow_connections.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Connections Panel can be accessed from the UI. from Admin -> Connections.\n",
    "\n",
    "Connections are fernet key encrypted after they're entered, prevent credentials from going into files, \n",
    "and can be used by other DAGs in the same instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not \"Airflowic\" Enough\n",
    "\n",
    "Using the hook with the PythonOperator cut down on repeated code, \n",
    "but the DAG file doesn't read like a config file yet.\n",
    "\n",
    "To polish it off, we'll write a custom FTPtoS3Operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
